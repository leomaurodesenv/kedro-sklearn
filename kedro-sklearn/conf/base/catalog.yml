# Here you can define all your data sets by using simple YAML syntax.
#
# Documentation for this file format can be found in "The Data Catalog"
# Link: https://docs.kedro.org/en/stable/data/data_catalog.html

_csv: &csv
  type: pandas.CSVDataSet
  load_args:
    sep: ','

_pickle: &pickle
  type: pickle.PickleDataSet
  backend: joblib

_json: &json
  type: json.JSONDataSet

## Dataset
train:
  <<: *csv
  filepath: data/01_raw/train.csv

test:
  <<: *csv
  filepath: data/01_raw/test.csv

sample_submission:
  <<: *csv
  filepath: data/01_raw/sample_submission.csv

submission:
  <<: *csv
  filepath: data/07_model_output/submission.csv

## Features
feature_vectorizer:
  <<: *pickle
  filepath: data/04_feature/vectorizer.joblib

## Models
model_svc:
  <<: *pickle
  filepath: data/06_models/model-svc.joblib

model_random_forest:
  <<: *pickle
  filepath: data/06_models/model-random_forest.joblib

model_logistic_regression:
  <<: *pickle
  filepath: data/06_models/model-logistic_regression.joblib

metrics_svc:
  <<: *json
  filepath: data/06_models/metrics-svc.json

metrics_random_forest:
  <<: *json
  filepath: data/06_models/metrics-random_forest.json

metrics_logistic_regression:
  <<: *json
  filepath: data/06_models/metrics-logistic_regression.json

## Selection
f1.selected_model:
  <<: *pickle
  filepath: data/06_models/selected_model-f1.pickle
  versioned: true

accuracy.selected_model:
  <<: *pickle
  filepath: data/06_models/selected_model-accuracy.pickle
  versioned: true
